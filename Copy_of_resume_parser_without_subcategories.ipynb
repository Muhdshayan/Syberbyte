{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhdshayan/Syberbyte/blob/Ai%2FLaiba/Copy_of_resume_parser_without_subcategories.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S_qU91HxqwP",
        "outputId": "436907db-d674-48ce-9836-57d033433d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q python-docx\n",
        "!pip install -q pymupdf\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import fitz  # PyMuPDF\n",
        "import docx\n",
        "import re\n",
        "import json\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "def read_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    return \"\\n\".join([page.get_text() for page in doc])\n",
        "\n",
        "def read_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "\n",
        "resume_text = \"\"\n",
        "for file_name in uploaded.keys():\n",
        "    if file_name.endswith(\".pdf\"):\n",
        "        resume_text = read_pdf(file_name)\n",
        "    elif file_name.endswith(\".docx\"):\n",
        "        resume_text = read_docx(file_name)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "print(\"✅ Resume Text Extracted:\\n\", resume_text[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "eLnG65O6zJ8-",
        "outputId": "bb5eee80-1d7a-4e77-c4ff-c545cd34da41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-995d72dd-97d6-4dcd-b91d-9eff1fbdf6bc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-995d72dd-97d6-4dcd-b91d-9eff1fbdf6bc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving LaibaKhan-1.pdf to LaibaKhan-1 (3).pdf\n",
            "✅ Resume Text Extracted:\n",
            " Laiba Khan \n",
            "Email: klaiba2406@gmail.com​ Contact No: +92 335 2036450​\n",
            "LinkedIn: https://www.linkedin.com/in/laibakhan00/​\n",
            "GitHub: https://github.com/laibak24 \n",
            " \n",
            " \n",
            "Academic Qualification \n",
            "2022 – Present: Bachelor's in Computer Science FAST - National University of Computer and Emerging Sciences (NUCES).  \n",
            "2020 – 2022: A-Levels (Computer Science) – The City School. \n",
            "2017 – 2020: O-Levels (Computer Science) – The City School. \n",
            " \n",
            " \n",
            "Technical Hands-on and Personal Skills \n",
            "●​\n",
            "Programming & Development: Python, Django, C++, C, JavaScript, React, React Native, HTML, CSS, SQL \n",
            "●​\n",
            "Machine Learning & AI: Building and deploying ML models, Natural Language Processing (NLP), Model Deployment \n",
            "(Streamlit, Vercel) , Chatbot Development & Training  \n",
            "●​\n",
            "Web & Mobile Development: Full-stack development with \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TECHNICAL SKILL DATABASE (COMPREHENSIVE) ---\n",
        "TECH_SKILL_DB = [\n",
        "    # Languages\n",
        "    \"python\", \"java\", \"c++\", \"c\", \"c#\", \"javascript\", \"typescript\", \"go\", \"rust\",\n",
        "    \"ruby\", \"swift\", \"kotlin\", \"scala\", \"matlab\", \"r\", \"sql\", \"bash\", \"shell\", \"php\",\n",
        "    \"perl\", \"haskell\", \"lua\", \"dart\", \"groovy\", \"objective-c\", \"assembly\",\n",
        "\n",
        "    # Web Dev\n",
        "    \"html\", \"css\", \"sass\", \"bootstrap\", \"tailwind\", \"react\", \"nextjs\", \"vue\", \"angular\",\n",
        "    \"express\", \"flask\", \"django\", \"fastapi\", \"webpack\", \"npm\", \"vite\", \"nuxtjs\", \"svelte\",\n",
        "    \"gatsby\", \"jquery\", \"d3js\", \"threejs\", \"redux\", \"mobx\", \"jest\", \"storybook\",\n",
        "\n",
        "    # Backend & API\n",
        "    \"nodejs\", \"spring\", \"dotnet\", \"graphql\", \"rest api\", \"soap\", \"grpc\", \"asp.net\",\n",
        "    \"laravel\", \"symfony\", \"ruby on rails\", \"play framework\", \"micronaut\", \"quarkus\",\n",
        "\n",
        "    # Databases\n",
        "    \"mysql\", \"postgresql\", \"mongodb\", \"sqlite\", \"firebase\", \"redis\", \"elasticsearch\",\n",
        "    \"dynamodb\", \"cassandra\", \"oracle\", \"mariadb\", \"cosmosdb\", \"neo4j\", \"arangodb\",\n",
        "    \"hbase\", \"couchdb\", \"realm\", \"firestore\",\n",
        "\n",
        "    # DevOps & Cloud\n",
        "    \"docker\", \"kubernetes\", \"aws\", \"azure\", \"gcp\", \"ci/cd\", \"jenkins\", \"github actions\",\n",
        "    \"terraform\", \"ansible\", \"linux\", \"nginx\", \"apache\", \"prometheus\", \"grafana\",\n",
        "    \"istio\", \"linkerd\", \"helm\", \"pulumi\", \"packer\", \"vagrant\", \"consul\", \"vault\",\n",
        "    \"cloudformation\", \"serverless\", \"lambda\", \"ec2\", \"s3\", \"rds\", \"cloudfront\",\n",
        "    \"azure functions\", \"azure devops\", \"google cloud functions\", \"gke\", \"aks\", \"eks\",\n",
        "\n",
        "    # AI/ML\n",
        "    \"machine learning\", \"deep learning\", \"ml\", \"dl\", \"nlp\", \"computer vision\",\n",
        "    \"tensorflow\", \"keras\", \"pytorch\", \"sklearn\", \"scikit-learn\", \"xgboost\", \"lightgbm\",\n",
        "    \"huggingface\", \"langchain\", \"transformers\", \"openai\", \"chatgpt\", \"llm\", \"llama\",\n",
        "    \"generative ai\", \"stable diffusion\", \"gan\", \"reinforcement learning\", \"time series\",\n",
        "    \"opencv\", \"spacy\", \"nltk\", \"bert\", \"gpt\", \"gemini\", \"claude\", \"mistral\",\n",
        "\n",
        "    # Data & Analysis\n",
        "    \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"plotly\", \"jupyter\", \"sqlalchemy\",\n",
        "    \"bigquery\", \"snowflake\", \"airflow\", \"spark\", \"hadoop\", \"kafka\", \"flink\", \"beam\",\n",
        "    \"dbt\", \"tableau\", \"power bi\", \"looker\", \"qlik\", \"metabase\", \"redshift\", \"databricks\",\n",
        "    \"hive\", \"presto\", \"trino\", \"clickhouse\", \"dask\", \"ray\",\n",
        "\n",
        "    # Business & Management\n",
        "    \"project management\", \"agile\", \"scrum\", \"kanban\", \"saas\", \"erp\", \"crm\", \"salesforce\",\n",
        "    \"sap\", \"jira\", \"confluence\", \"trello\", \"asana\", \"clickup\", \"ms project\", \"okrs\",\n",
        "    \"kpis\", \"business intelligence\", \"market analysis\", \"financial modeling\", \"risk management\",\n",
        "    \"supply chain\", \"logistics\", \"six sigma\", \"lean\", \"process improvement\", \"digital transformation\",\n",
        "\n",
        "    # Cybersecurity\n",
        "    \"cybersecurity\", \"information security\", \"penetration testing\", \"ethical hacking\",\n",
        "    \"owasp\", \"siem\", \"soc\", \"vulnerability assessment\", \"pki\", \"ssl/tls\", \"firewalls\",\n",
        "    \"ids/ips\", \"iso 27001\", \"gdpr\", \"pci dss\", \"nist\", \"cryptography\", \"zero trust\",\n",
        "\n",
        "    # Blockchain\n",
        "    \"blockchain\", \"smart contracts\", \"solidity\", \"web3\", \"ethereum\", \"hyperledger\",\n",
        "    \"defi\", \"nft\", \"cryptocurrency\", \"bitcoin\", \"consensus algorithms\", \"ipfs\",\n",
        "\n",
        "    # Embedded & IoT\n",
        "    \"embedded systems\", \"arduino\", \"raspberry pi\", \"iot\", \"fpga\", \"verilog\", \"vhd\",\n",
        "    \"rtos\", \"microcontrollers\", \"bluetooth\", \"zigbee\", \"mqtt\", \"modbus\", \"can bus\",\n",
        "\n",
        "    # Game Dev\n",
        "    \"unity\", \"unreal engine\", \"godot\", \"cryengine\", \"game design\", \"3d modeling\",\n",
        "    \"blender\", \"maya\", \"opengl\", \"directx\", \"vulkan\", \"shaders\", \"physics engines\",\n",
        "\n",
        "    # Testing\n",
        "    \"pytest\", \"unittest\", \"jest\", \"mocha\", \"cypress\", \"selenium\", \"jmeter\", \"junit\",\n",
        "    \"testng\", \"cucumber\", \"postman\", \"soapui\", \"load testing\", \"stress testing\",\n",
        "    \"security testing\", \"qa automation\", \"test driven development\",\n",
        "\n",
        "    # Mobile Dev\n",
        "    \"android\", \"ios\", \"flutter\", \"react native\", \"swiftui\", \"xcode\", \"android studio\",\n",
        "    \"kotlin multiplatform\", \"jetpack compose\", \"mvvm\", \"mobile ui/ux\", \"apk\",\n",
        "    \"google play\", \"app store\", \"push notifications\", \"in-app purchases\",\n",
        "\n",
        "    # Design & Multimedia\n",
        "    \"ui/ux\", \"figma\", \"adobe xd\", \"sketch\", \"photoshop\", \"illustrator\", \"premiere pro\",\n",
        "    \"after effects\", \"motion graphics\", \"video editing\", \"3d animation\", \"coreldraw\",\n",
        "    \"indesign\", \"prototyping\", \"wireframing\", \"user research\", \"accessibility\",\n",
        "\n",
        "    # Networking\n",
        "    \"tcp/ip\", \"dns\", \"dhcp\", \"vpn\", \"wan\", \"lan\", \"sdn\", \"ospf\", \"bgp\", \"mpls\",\n",
        "    \"voip\", \"sip\", \"qos\", \"network security\", \"wireshark\", \"packet analysis\",\n",
        "\n",
        "    # Hardware\n",
        "    \"computer architecture\", \"cpu design\", \"gpu programming\", \"cuda\", \"opencl\",\n",
        "    \"embedded linux\", \"device drivers\", \"raspberry pi\", \"circuit design\", \"pcb\",\n",
        "    \"altium\", \"cad\", \"vlsi\", \"asic\", \"fpga programming\",\n",
        "\n",
        "    # Quantum Computing\n",
        "    \"quantum computing\", \"qiskit\", \"cirq\", \"quantum algorithms\", \"quantum cryptography\",\n",
        "\n",
        "    # Miscellaneous Tech\n",
        "    \"arduino\", \"raspberry pi\", \"robotics\", \"computer graphics\", \"gis\", \"autocad\",\n",
        "    \"solidworks\", \"ansys\", \"industrial automation\", \"plc\", \"scada\"\n",
        "]\n",
        "\n",
        "# --- SOFT SKILLS DATABASE ---\n",
        "SOFT_SKILL_DB = [\n",
        "    \"communication\", \"teamwork\", \"collaboration\", \"leadership\", \"problem solving\",\n",
        "    \"critical thinking\", \"adaptability\", \"creativity\", \"emotional intelligence\",\n",
        "    \"time management\", \"organization\", \"attention to detail\", \"work ethic\",\n",
        "    \"interpersonal skills\", \"negotiation\", \"conflict resolution\", \"decision making\",\n",
        "    \"strategic thinking\", \"analytical skills\", \"presentation skills\", \"public speaking\",\n",
        "    \"active listening\", \"empathy\", \"patience\", \"diplomacy\", \"mentoring\", \"coaching\",\n",
        "    \"networking\", \"relationship building\", \"cultural awareness\", \"diversity and inclusion\",\n",
        "    \"persuasion\", \"influence\", \"delegation\", \"accountability\", \"reliability\", \"initiative\",\n",
        "    \"self-motivation\", \"resilience\", \"stress management\", \"growth mindset\", \"curiosity\",\n",
        "    \"continuous learning\", \"flexibility\", \"multitasking\", \"prioritization\", \"goal setting\",\n",
        "    \"feedback\", \"constructive criticism\", \"transparency\", \"professionalism\", \"business etiquette\",\n",
        "    \"customer service\", \"client management\", \"stakeholder management\", \"project coordination\",\n",
        "    \"change management\", \"risk assessment\", \"innovation\", \"design thinking\", \"agile mindset\",\n",
        "    \"remote collaboration\", \"virtual teamwork\", \"written communication\", \"verbal communication\",\n",
        "    \"nonverbal communication\", \"storytelling\", \"data-driven decision making\", \"resourcefulness\",\n",
        "    \"self-awareness\", \"self-regulation\", \"motivation\", \"social skills\", \"positive attitude\",\n",
        "    \"humor\", \"tact\", \"discretion\", \"ethical judgment\", \"corporate governance\"\n",
        "]"
      ],
      "metadata": {
        "id": "KhAf2XIIziN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONTACT INFORMATION EXTRACTION ---\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"Extracts validated contact information: phone, email, LinkedIn, and GitHub.\"\"\"\n",
        "    contact_info = {\n",
        "        \"phone\": \"\",\n",
        "        \"email\": \"\",\n",
        "        \"linkedin\": \"\",\n",
        "        \"github\": \"\"\n",
        "    }\n",
        "\n",
        "    # PHONE: Sanity check for valid formats and digit count\n",
        "    phone_pattern = r'(?:(?:\\+|00)?[0-9]{1,3}[\\s\\-]?)?(?:\\(?\\d{2,4}\\)?[\\s\\-]?)?\\d{3,4}[\\s\\-]?\\d{3,4}'\n",
        "    phones = re.findall(phone_pattern, text)\n",
        "    valid_phones = [p.strip() for p in phones if len(re.sub(r'\\D', '', p)) >= 10]\n",
        "    if valid_phones:\n",
        "        contact_info[\"phone\"] = valid_phones[0]\n",
        "\n",
        "    # EMAIL: Basic structure + common domain check\n",
        "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    if emails and all(c not in emails[0] for c in [\" \", \",\", \";\"]):\n",
        "        contact_info[\"email\"] = emails[0].lower()\n",
        "\n",
        "    # LINKEDIN: Normalize format\n",
        "    linkedin_pattern = r'https?:\\/\\/(www\\.)?linkedin\\.com\\/[a-zA-Z0-9\\/\\-_]+'\n",
        "    linkedin = re.search(linkedin_pattern, text)\n",
        "    if linkedin:\n",
        "        contact_info[\"linkedin\"] = linkedin.group(0).strip()\n",
        "\n",
        "    # GITHUB: Normalize format\n",
        "    github_pattern = r'https?:\\/\\/(www\\.)?github\\.com\\/[a-zA-Z0-9\\-_]+'\n",
        "    github = re.search(github_pattern, text)\n",
        "    if github:\n",
        "        contact_info[\"github\"] = github.group(0).strip()\n",
        "\n",
        "    return contact_info\n",
        "\n",
        "def extract_section(text, section_title):\n",
        "    \"\"\"\n",
        "    Extract section by title and return clean content.\n",
        "    Supports flexible title matching with common variations.\n",
        "    \"\"\"\n",
        "    # Common section title variations mapping\n",
        "    section_aliases = {\n",
        "        \"education\": [\"education\", \"academics\", \"educational background\"],\n",
        "        \"experience\": [\"experience\", \"work history\", \"professional experience\"],\n",
        "        \"skills\": [\"skills\", \"technical skills\", \"key skills\"],\n",
        "        \"projects\": [\"projects\", \"key projects\", \"personal projects\"],\n",
        "        \"certifications\": [\"certifications\", \"certificates\", \"courses and certifications\"],\n",
        "        \"soft skills\": [\"soft skills\", \"interpersonal skills\", \"strengths\"]\n",
        "    }\n",
        "\n",
        "    # Find the normalized section title\n",
        "    normalized_title = None\n",
        "    for key, aliases in section_aliases.items():\n",
        "        if section_title.lower() in [key.lower()] + [a.lower() for a in aliases]:\n",
        "            normalized_title = key\n",
        "            break\n",
        "\n",
        "    if not normalized_title:\n",
        "        normalized_title = section_title\n",
        "\n",
        "    # Build flexible regex pattern\n",
        "    title_pattern = \"|\".join([re.escape(t) for t in [normalized_title] + section_aliases.get(normalized_title, [])])\n",
        "    pattern = rf\"(?i)(?:{title_pattern}).*?(?=\\n\\s*\\n|\\n[A-Z][A-Za-z ]+(?:\\n|$)|\\Z)\"\n",
        "\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        section = match.group(0).strip()\n",
        "        # Remove section header and any following punctuation/whitespace\n",
        "        section = re.sub(rf\"^(?:{title_pattern})[\\s:.-]*\", \"\", section, flags=re.IGNORECASE)\n",
        "        return section.strip()\n",
        "    return \"\"\n",
        "\n",
        "def extract_resume_sections(resume_text):\n",
        "    \"\"\"\n",
        "    Extract all relevant sections from resume text with improved normalization.\n",
        "    Returns a dictionary with consistent section names regardless of input variations.\n",
        "    \"\"\"\n",
        "    sections = {\n",
        "        \"education\": extract_section(resume_text, \"Education\"),\n",
        "        \"experience\": extract_section(resume_text, \"Experience\"),\n",
        "        \"skills\": extract_section(resume_text, \"Skills\"),\n",
        "        \"projects\": extract_section(resume_text, \"Projects\"),\n",
        "        \"certifications\": extract_section(resume_text, \"Certifications\"),\n",
        "        \"soft_skills\": extract_section(resume_text, \"Soft Skills\")\n",
        "    }\n",
        "\n",
        "    # Post-processing for common patterns\n",
        "    for section, content in sections.items():\n",
        "        # Remove empty lines and excessive whitespace\n",
        "        content = \"\\n\".join([line.strip() for line in content.splitlines() if line.strip()])\n",
        "        # Remove bullet points and other markers\n",
        "        content = re.sub(r\"^[\\s•\\-*]\\s*\", \"\", content, flags=re.MULTILINE)\n",
        "        sections[section] = content.strip()\n",
        "\n",
        "    return sections\n",
        "def extract_skills(text, skill_db):\n",
        "    \"\"\"\n",
        "    Extracts unique skills from a text by matching with a lowercased skill DB.\n",
        "    Includes basic word boundary matching to avoid partial matches.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    found = []\n",
        "    for skill in skill_db:\n",
        "        if re.search(rf\"\\b{re.escape(skill.lower())}\\b\", text_lower):\n",
        "            found.append(skill)\n",
        "    return list(set(found))\n",
        "\n",
        "# --- NAME AND DATE EXTRACTION ---\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(resume_text)\n",
        "import re\n",
        "\n",
        "def extract_soft_skills(text, soft_skill_db):\n",
        "    \"\"\"\n",
        "    Extracts unique soft skills from text by matching with a soft skill database.\n",
        "    Uses flexible matching to account for common variations in soft skill phrasing.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to analyze (resume, job description, etc.)\n",
        "        soft_skill_db: List of soft skills to match against\n",
        "\n",
        "    Returns:\n",
        "        List of unique soft skills found in the text\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    found = []\n",
        "\n",
        "    for skill in soft_skill_db:\n",
        "        skill_lower = skill.lower()\n",
        "        # Create more flexible patterns for multi-word skills\n",
        "        pattern = r\"(^|\\W)\" + re.escape(skill_lower) + r\"($|\\W)\"\n",
        "        if re.search(pattern, text_lower):\n",
        "            found.append(skill)\n",
        "\n",
        "    # Additional processing for common variations\n",
        "    variations = {\n",
        "        \"teamwork\": [\"team player\", \"team-player\", \"team work\"],\n",
        "        \"communication\": [\"communication skills\", \"communicate effectively\"],\n",
        "        \"problem solving\": [\"problem-solve\", \"solving problems\"],\n",
        "        \"time management\": [\"manage time\", \"time-manage\"]\n",
        "    }\n",
        "\n",
        "    for canonical_skill, aliases in variations.items():\n",
        "        if canonical_skill in found:\n",
        "            continue\n",
        "        for alias in aliases:\n",
        "            if re.search(rf\"\\b{re.escape(alias)}\\b\", text_lower):\n",
        "                found.append(canonical_skill)\n",
        "                break\n",
        "\n",
        "    return sorted(list(set(found)), key=lambda x: x.lower())\n",
        "\n",
        "def extract_name(resume_text, entities, skill_db):\n",
        "    \"\"\"\n",
        "    Enhanced name extraction that better filters out section headers and other non-name text\n",
        "    \"\"\"\n",
        "    # Common false positives to exclude\n",
        "    NON_NAME_PATTERNS = [\n",
        "        r'academic', r'qualification', r'education', r'experience',\n",
        "        r'skill', r'project', r'objective', r'summary',\n",
        "        r'curriculum vitae', r'resume', r'cv', r'contact'\n",
        "    ]\n",
        "\n",
        "    # Pre-process skill_db to lowercase for case-insensitive comparison\n",
        "    skill_db_lower = [skill.lower() for skill in skill_db]\n",
        "\n",
        "    # Method 1: Use spaCy's PERSON entities with strict validation\n",
        "    if \"PERSON\" in entities:\n",
        "        for name in entities[\"PERSON\"]:\n",
        "            name_clean = name.strip().title()\n",
        "            name_parts = name_clean.split()\n",
        "\n",
        "            # Strong validation checks\n",
        "            if (2 <= len(name_parts) <= 3 and                     # 2-3 name parts\n",
        "                all(len(part) > 1 for part in name_parts) and     # each part >1 char\n",
        "                all(part.isalpha() for part in name_parts) and    # alphabetic only\n",
        "                not any(re.search(pattern, name_clean.lower()) for pattern in NON_NAME_PATTERNS) and\n",
        "                name_clean.lower() not in skill_db_lower):        # not in skills\n",
        "                return name_clean\n",
        "\n",
        "    # Method 2: Scan first 15 non-empty lines for likely names\n",
        "    lines = [line.strip() for line in resume_text.split('\\n') if line.strip()][:15]\n",
        "    for line in lines:\n",
        "        line_clean = line.strip()\n",
        "        line_parts = line_clean.split()\n",
        "\n",
        "        # Skip if doesn't look like a name\n",
        "        if (len(line_clean) > 40 or                 # too long\n",
        "           len(line_clean) < 5 or                   # too short\n",
        "           len(line_parts) not in [2, 3] or         # not 2-3 parts\n",
        "           not line_clean == line_clean.title() or  # not title case\n",
        "           any(c.isdigit() for c in line_clean) or # contains numbers\n",
        "           any(not word.isalpha() for word in line_parts) or # non-alpha\n",
        "           any(word.lower() in skill_db_lower for word in line_parts) or # in skills\n",
        "           any(re.search(pattern, line_clean.lower()) for pattern in NON_NAME_PATTERNS)):\n",
        "            continue\n",
        "\n",
        "        return line_clean\n",
        "\n",
        "    # Method 3: Extract from email (first.last@domain)\n",
        "    email_match = re.search(r'([a-zA-Z]+)[\\._]([a-zA-Z]+)@', resume_text)\n",
        "    if email_match:\n",
        "        first = email_match.group(1).title()\n",
        "        last = email_match.group(2).title()\n",
        "        if (len(first) > 1 and len(last) > 1 and\n",
        "            first.lower() not in skill_db_lower and\n",
        "            last.lower() not in skill_db_lower):\n",
        "            return f\"{first} {last}\"\n",
        "\n",
        "    # Method 4: Extract from LinkedIn URL\n",
        "    linkedin_match = re.search(r'linkedin\\.com/(?:in|pub)/([a-zA-Z-]+)-?([a-zA-Z-]+)', resume_text.lower())\n",
        "    if linkedin_match:\n",
        "        first = linkedin_match.group(1).title()\n",
        "        last = linkedin_match.group(2).title()\n",
        "        if (len(first) > 1 and len(last) > 1 and\n",
        "            first.lower() not in skill_db_lower and\n",
        "            last.lower() not in skill_db_lower):\n",
        "            return f\"{first} {last}\"\n",
        "\n",
        "    return \"Name Not Found\""
      ],
      "metadata": {
        "id": "UkAJwAfvXSwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_professional_summary(text):\n",
        "    \"\"\"\n",
        "    Extracts the professional summary or career objective from the resume.\n",
        "    Uses multiple fallback strategies to improve accuracy.\n",
        "    \"\"\"\n",
        "    summary_aliases = [\n",
        "        \"professional summary\", \"summary\", \"career summary\", \"about me\", \"profile\",\n",
        "        \"objective\", \"career objective\", \"personal statement\"\n",
        "    ]\n",
        "\n",
        "    # --- Strategy 1: Section-based extraction (most accurate)\n",
        "    for alias in summary_aliases:\n",
        "        summary = extract_section(text, alias)\n",
        "        if summary and len(summary.split()) >= 10:\n",
        "            return summary.strip()\n",
        "\n",
        "    # --- Strategy 2: Heuristic-based fallback (first 10 lines)\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    for i in range(min(10, len(lines))):\n",
        "        line = lines[i]\n",
        "        if any(start in line.lower() for start in [\"i am\", \"i have\", \"results-driven\", \"seasoned\", \"experienced\", \"seeking a\", \"motivated\", \"detail-oriented\"]):\n",
        "            # Capture up to 3 lines if they seem part of a paragraph\n",
        "            summary_lines = [line]\n",
        "            for j in range(i+1, min(i+4, len(lines))):\n",
        "                if len(lines[j].split()) < 3: break\n",
        "                summary_lines.append(lines[j])\n",
        "            summary = \" \".join(summary_lines)\n",
        "            if len(summary.split()) >= 10:\n",
        "                return summary.strip()\n",
        "\n",
        "    # --- Strategy 3: Regex match for typical summary phrasing\n",
        "    regex_patterns = [\n",
        "        r\"(?:experienced|motivated|results[- ]?oriented|driven|seasoned)[^.]{20,250}\\.\",\n",
        "        r\"(?:seeking a|aiming for|looking for)[^.]{20,250}\\.\"\n",
        "    ]\n",
        "    for pattern in regex_patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(0).strip()\n",
        "\n",
        "    return \"Summary Not Found\"\n"
      ],
      "metadata": {
        "id": "R8Ds33NXwIdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    \"\"\"\n",
        "    Extracts education information and excludes other sections like licenses, certificates, etc.\n",
        "    Returns a list of combined education strings in format: \"Degree Institution (Year)\"\n",
        "    \"\"\"\n",
        "    education_entries = []\n",
        "\n",
        "    # Normalize text - replace different dash types with standard hyphen\n",
        "    text = re.sub(r'[–—]', '-', resume_text)\n",
        "\n",
        "    # Common education degree patterns (to help filter out non-education entries)\n",
        "    degree_patterns = [\n",
        "        r'\\bB\\.?A\\.?\\b', r'\\bB\\.?S\\.?\\b', r'\\bB\\.?E\\.?\\b', r'\\bB\\.?Tech\\b',\n",
        "        r'\\bM\\.?S\\.?\\b', r'\\bM\\.?A\\.?\\b', r'\\bM\\.?Tech\\b', r'\\bM\\.?E\\.?\\b',\n",
        "        r'\\bPh\\.?D\\.?\\b', r'\\bBachelor\\b', r'\\bMaster\\b', r'\\bDoctorate\\b',\n",
        "        r'\\bDiploma\\b', r'\\bAssociate\\b', r'\\bHigh School\\b', r'\\bSecondary\\b',\n",
        "        r'\\bH\\.?S\\.?\\b'\n",
        "    ]\n",
        "    degree_regex = re.compile('|'.join(degree_patterns), re.IGNORECASE)\n",
        "\n",
        "    # First extract the education section with stricter boundaries\n",
        "    education_section = re.search(\n",
        "        r'(?i)(?:education|academic qualification|academic background|qualifications?)[\\s:]*\\n?(.*?)'\n",
        "        r'(?=(?:\\n\\s*\\n)|(?:\\n(?:experience|work history|projects|certifications|licenses|skills)\\b)|$)',\n",
        "        text, re.DOTALL\n",
        "    )\n",
        "\n",
        "    text_to_scan = education_section.group(1).strip() if education_section else \"\"\n",
        "\n",
        "    if not text_to_scan:\n",
        "        return education_entries\n",
        "\n",
        "    # Split into individual entries - more robust splitting\n",
        "    entries = re.split(r'\\n(?=\\s*(?:\\d{4}\\s*[-–—]\\s*(?:Present|\\d{4})|[A-Z][a-z]+))|\\n\\s*\\n', text_to_scan)\n",
        "\n",
        "    # Process each entry\n",
        "    for entry in entries:\n",
        "        entry = entry.strip()\n",
        "        if not entry:\n",
        "            continue\n",
        "\n",
        "        # Skip if contains common non-education keywords\n",
        "        non_edu_keywords = [\n",
        "            'certificate', 'certification', 'license', 'project',\n",
        "            'training', 'workshop', 'seminar', 'conference'\n",
        "        ]\n",
        "        if any(re.search(rf'\\b{kw}\\b', entry, re.I) for kw in non_edu_keywords):\n",
        "            continue\n",
        "\n",
        "        # Initialize components\n",
        "        degree = ''\n",
        "        institution = ''\n",
        "        year = ''\n",
        "\n",
        "        # Extract year range - more flexible pattern\n",
        "        year_match = re.search(\n",
        "            r'(\\d{4}\\s*[-–—]\\s*(?:Present|\\d{4})|\\b(?:20|19)\\d{2}\\b)',\n",
        "            entry\n",
        "        )\n",
        "        if year_match:\n",
        "            year = year_match.group(1).strip()\n",
        "            entry = entry.replace(year_match.group(1), '').strip()\n",
        "\n",
        "        # Split degree and institution - handle multiple separators\n",
        "        parts = re.split(r'[-–—:•]', entry, maxsplit=1)\n",
        "        if len(parts) == 2:\n",
        "            degree = parts[0].strip()\n",
        "            institution = parts[1].strip()\n",
        "        else:\n",
        "            degree = entry.strip()\n",
        "\n",
        "        # Clean up degree - remove any remaining year references\n",
        "        degree = re.sub(r'[.,]?\\s*\\d{4}.*$', '', degree).strip()\n",
        "\n",
        "        # Clean up institution - remove trailing periods and years\n",
        "        if institution:\n",
        "            institution = re.sub(r'[.,]\\s*\\d{4}.*$', '', institution).strip()\n",
        "\n",
        "        # Validate this is actually an education entry\n",
        "        is_education = (\n",
        "            degree_regex.search(degree) or\n",
        "            any(word in institution.lower() for word in ['university', 'college', 'institute', 'school'])\n",
        "        )\n",
        "\n",
        "        if not is_education:\n",
        "            continue\n",
        "\n",
        "        # Combine all components into a single string\n",
        "        combined = []\n",
        "        if degree:\n",
        "            combined.append(degree)\n",
        "        if institution:\n",
        "            combined.append(institution)\n",
        "        if year:\n",
        "            combined.append(f\"({year})\")\n",
        "\n",
        "        # Only add if we have meaningful data\n",
        "        if combined:\n",
        "            education_entries.append(' '.join(combined))\n",
        "\n",
        "    return education_entries"
      ],
      "metadata": {
        "id": "Gu3LHOtc2Eeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_experience(resume_text):\n",
        "    \"\"\"\n",
        "    Extracts work experience from resume text and returns combined entries as strings.\n",
        "    Format: \"Role at Organization (Duration)\"\n",
        "    Returns at least one entry if any experience is found.\n",
        "    \"\"\"\n",
        "    experiences = []\n",
        "\n",
        "    # Normalize text\n",
        "    text = re.sub(r'\\t', ' ', resume_text)  # Convert tabs to spaces\n",
        "    text = re.sub(r'[–—]', '-', text)  # Standardize dashes\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "\n",
        "    # Extract experience section with strict boundaries\n",
        "    exp_section_match = re.search(\n",
        "        r'(?i)(?:work\\s*experience|professional\\s*experience|employment\\s*history|'\n",
        "        r'internship\\s*experience|internships?|experience|relevant\\s*experience|'\n",
        "        r'career\\s*history|work)(?!.*\\bskills\\b)[\\s:]*\\n?(.*?)(?=\\n\\s*(?:education|skills|projects|'\n",
        "        r'awards|publications|volunteer|personal|references|summary|about|interests|'\n",
        "        r'achievements|certifications|licenses|technical\\s*skills)[\\s:]|\\Z)',\n",
        "        text,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    exp_section = exp_section_match.group(1).strip() if exp_section_match else \"\"\n",
        "\n",
        "    # If no section found, try scanning entire text with experience keywords\n",
        "    if not exp_section:\n",
        "        exp_lines = []\n",
        "        for line in text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if (re.search(r'(?i)\\b(?:intern|worked|employed|experience|analyst|engineer|developer|'\n",
        "                         r'manager|director|consultant|specialist)\\b', line) and\n",
        "                not re.search(r'(?i)\\b(?:project|certificate|education|course)\\b', line)):\n",
        "                exp_lines.append(line)\n",
        "        exp_section = '\\n'.join(exp_lines)\n",
        "\n",
        "    if not exp_section:\n",
        "        return [\"Work experience not specified\"] if not experiences else experiences\n",
        "\n",
        "    # Split into individual entries\n",
        "    entries = re.split(r'\\n\\s*\\n|\\n(?=\\s*(?:[A-Z][a-z]+|[\\w+]+\\s+at\\s+|\\d{4}|•|-|\\*))', exp_section)\n",
        "\n",
        "    for entry in entries:\n",
        "        entry = entry.strip()\n",
        "        if not entry or len(entry) < 15:  # Minimum reasonable length\n",
        "            continue\n",
        "\n",
        "        # Skip non-experience entries\n",
        "        if re.search(r'(?i)(email|contact|phone|linkedin|github|@|\\+\\d|^[A-Z]+\\s*$|'\n",
        "                    r'certificat|license|project|course|training|workshop|seminar|'\n",
        "                    r'education|degree|skill|hobby)', entry):\n",
        "            continue\n",
        "\n",
        "        # Initialize components\n",
        "        role = ''\n",
        "        organization = ''\n",
        "        duration = ''\n",
        "\n",
        "        # Extract duration (most reliable anchor)\n",
        "        duration_match = re.search(\n",
        "            r'(\\(?\\s*(?:January|February|March|April|May|June|July|August|September|'\n",
        "            r'October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*'\n",
        "            r'[,.\\s-]*(?:\\d{4}|\\d{1,2})?\\s*(?:-|to|–)\\s*(?:Present|Now|Current|'\n",
        "            r'(?:January|February|March|April|May|June|July|August|September|October|'\n",
        "            r'November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*'\n",
        "            r'[,.\\s-]*(?:\\d{4}|\\d{1,2})?)\\s*\\)?|\\d{4}\\s*(?:-|to|–)\\s*(?:Present|Now|Current|\\d{4}))',\n",
        "            entry,\n",
        "            re.I\n",
        "        )\n",
        "        if duration_match:\n",
        "            duration = duration_match.group(1).strip('() ')\n",
        "            entry = entry.replace(duration_match.group(1), '').strip()\n",
        "\n",
        "        # Extract role and organization\n",
        "        # Case 1: \"Role at Organization\" format\n",
        "        at_match = re.search(r'(.+?)\\s+at\\s+(.+)', entry, re.I)\n",
        "        if at_match:\n",
        "            role = at_match.group(1).strip()\n",
        "            organization = at_match.group(2).strip()\n",
        "        else:\n",
        "            # Case 2: \"Role, Organization\" or \"Role - Organization\"\n",
        "            parts = re.split(r'[,–—\\-•]', entry, maxsplit=1)\n",
        "            if len(parts) >= 2:\n",
        "                role = parts[0].strip()\n",
        "                organization = parts[1].strip()\n",
        "            else:\n",
        "                # Case 3: Just role (last resort)\n",
        "                role = entry.strip()\n",
        "\n",
        "        # Clean organization name\n",
        "        if organization:\n",
        "            organization = re.sub(r'[,\\-–—].*$', '', organization).strip()\n",
        "            organization = re.sub(r'(?i)\\b(?:present|current)\\b.*$', '', organization).strip()\n",
        "\n",
        "        # Validate this is actually an experience entry\n",
        "        is_experience = (\n",
        "            bool(role) and\n",
        "            len(role.split()) <= 5 and  # Role shouldn't be too long\n",
        "            (bool(organization) or bool(duration)) and\n",
        "            not any(word in role.lower() for word in ['project', 'certificate', 'course', 'skill'])\n",
        "        )\n",
        "\n",
        "        if not is_experience:\n",
        "            continue\n",
        "\n",
        "        # Combine components\n",
        "        combined = []\n",
        "        if role:\n",
        "            combined.append(role)\n",
        "        if organization:\n",
        "            combined.append(f\"at {organization}\")\n",
        "        if duration:\n",
        "            combined.append(f\"({duration})\")\n",
        "\n",
        "        if combined:\n",
        "            exp_str = ' '.join(combined)\n",
        "            # Final validation - must contain at least role + one other component\n",
        "            if len(combined) >= 2:\n",
        "                experiences.append(exp_str)\n",
        "\n",
        "    # Fallback if no structured experience found but section exists\n",
        "    if not experiences and exp_section:\n",
        "        # Try to extract just the first line as experience\n",
        "        first_line = exp_section.split('\\n')[0].strip()\n",
        "        if len(first_line) > 10:  # Minimum reasonable length\n",
        "            experiences.append(first_line)\n",
        "\n",
        "    return experiences if experiences else [\"Work experience not specified\"]"
      ],
      "metadata": {
        "id": "M6fP9wsV-1pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_projects(resume_text):\n",
        "    \"\"\"\n",
        "    Extracts project information from resume text and returns a list of projects.\n",
        "    Each project is a dictionary with all details in a single 'content' field.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        projects = []\n",
        "\n",
        "        # Normalize text\n",
        "        text = re.sub(r'[–—]', '-', resume_text)\n",
        "\n",
        "        # Find the projects section with flexible headers and stopping points\n",
        "        projects_section_match = re.search(\n",
        "            r'(?i)(Projects|Personal Projects|Academic Projects|Project Details)\\s*\\n([\\s\\S]*?)(?=\\n\\s*(?:Courses|Certificates|Education|Skills|Experience|Work|Employment|Achievements|$))',\n",
        "            text\n",
        "        )\n",
        "\n",
        "        if not projects_section_match:\n",
        "            return []\n",
        "\n",
        "        projects_section = projects_section_match.group(2).strip()\n",
        "\n",
        "        # Split projects - handles various formats (bullet points, numbered, dashed, etc.)\n",
        "        project_entries = re.split(r'\\n(?=\\s*(?:•|\\d+\\.|[-*]|\\w.+?:|Project\\s*\\d*:|[A-Z][a-z]+))', projects_section)\n",
        "\n",
        "        for entry in project_entries:\n",
        "            entry = entry.strip()\n",
        "            if not entry or len(entry) < 20:  # Minimum reasonable project length\n",
        "                continue\n",
        "\n",
        "            # Clean up the project content\n",
        "            content = ' '.join(line.strip() for line in entry.split('\\n') if line.strip())\n",
        "            content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "            # Skip if it looks like a section header accidentally captured\n",
        "            if re.match(r'^(?:Projects?|Skills?|Technologies?):?$', content, re.I):\n",
        "                continue\n",
        "\n",
        "            # Create simple project entry with all content\n",
        "            projects.append({\n",
        "                'content': content\n",
        "            })\n",
        "\n",
        "        return projects\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Project extraction error: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "YQbEbcSIR53d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_courses_certificates(resume_text):\n",
        "    \"\"\"\n",
        "    Extracts course and certification names from sections like\n",
        "    'Certifications', 'Courses', 'Relevant Coursework', etc.\n",
        "    Returns a list of clean certificate/course names.\n",
        "    \"\"\"\n",
        "    lines = resume_text.splitlines()\n",
        "    course_lines = []\n",
        "    capture = False\n",
        "\n",
        "    section_headers = ['certifications', 'certification', 'courses', 'coursework', 'relevant coursework']\n",
        "    stop_headers = ['experience', 'education', 'projects', 'skills', 'internship', 'achievements']\n",
        "\n",
        "    for line in lines:\n",
        "        line_clean = line.strip()\n",
        "        line_lower = line_clean.lower()\n",
        "\n",
        "        # Start of certificate/course section\n",
        "        if any(header in line_lower for header in section_headers):\n",
        "            capture = True\n",
        "            continue\n",
        "\n",
        "        # Stop when next major section starts\n",
        "        if any(header in line_lower for header in stop_headers):\n",
        "            capture = False\n",
        "\n",
        "        if capture and line_clean:\n",
        "            course_lines.append(line_clean)\n",
        "\n",
        "    # Clean and extract course names only\n",
        "    certificates = []\n",
        "    for line in course_lines:\n",
        "        # Remove any date or extra trailing info in brackets or after dash\n",
        "        line = re.sub(r'\\(.*?\\)', '', line)  # Remove parentheses content\n",
        "        line = re.split(r'[-–:|]', line)[0]  # Split by separator, keep course name\n",
        "        course_name = line.strip()\n",
        "        if 3 <= len(course_name) <= 100:  # Reasonable length\n",
        "            certificates.append(course_name)\n",
        "\n",
        "    return certificates\n"
      ],
      "metadata": {
        "id": "g9J27QMHMgIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# --- ENTITY EXTRACTION ---\n",
        "def get_entities(text):\n",
        "    entities = {\"PERSON\": [], \"ORG\": [], \"DATE\": [], \"GPE\": []}\n",
        "    try:\n",
        "        import spacy\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        doc = nlp(text)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in entities:\n",
        "                entities[ent.label_].append(ent.text.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ spaCy entity recognition error: {str(e)}\")\n",
        "    return entities\n",
        "\n",
        "# --- PROFILE CONSTRUCTION ---\n",
        "def build_profile(resume_text):\n",
        "    entities = get_entities(resume_text)\n",
        "    return {\n",
        "        \"name\": extract_name(resume_text, entities, TECH_SKILL_DB),\n",
        "        \"contact\": extract_contact_info(resume_text),\n",
        "        \"summary\": extract_professional_summary(resume_text),\n",
        "        \"skills\": {\n",
        "            \"technical\": extract_skills(resume_text, TECH_SKILL_DB),\n",
        "            \"soft\": extract_soft_skills(resume_text, SOFT_SKILL_DB)\n",
        "        },\n",
        "        \"education\": extract_education(resume_text),\n",
        "        \"experience\": extract_experience(resume_text),\n",
        "        \"projects\": extract_projects(resume_text),\n",
        "        \"certifications\": extract_courses_certificates(resume_text)\n",
        "    }\n",
        "\n",
        "def build_error_profile(error, resume_text=None):\n",
        "    profile_data = {\n",
        "        \"name\": None,\n",
        "        \"contact\": {},\n",
        "        \"summary\": None,\n",
        "        \"skills\": {\"technical\": [], \"soft\": []},\n",
        "        \"education\": [],\n",
        "        \"experience\": [],\n",
        "        \"projects\": [],\n",
        "        \"certifications\": []\n",
        "    }\n",
        "\n",
        "    if resume_text:\n",
        "        entities = get_entities(resume_text)\n",
        "        profile_data.update({\n",
        "            \"name\": extract_name(resume_text, entities, TECH_SKILL_DB),\n",
        "            \"contact\": extract_contact_info(resume_text),\n",
        "            \"summary\": extract_professional_summary(resume_text),\n",
        "            \"skills\": {\n",
        "                \"technical\": extract_skills(resume_text, TECH_SKILL_DB),\n",
        "                \"soft\": extract_soft_skills(resume_text, SOFT_SKILL_DB)\n",
        "            },\n",
        "            \"education\": extract_education(resume_text),\n",
        "            \"experience\": extract_experience(resume_text),\n",
        "            \"projects\": extract_projects(resume_text),\n",
        "            \"certifications\": extract_courses_certificates(resume_text)\n",
        "        })\n",
        "\n",
        "    return profile_data\n",
        "\n",
        "# --- OUTPUT ---\n",
        "def save_profile_to_file(profile, filename=None):\n",
        "    try:\n",
        "        if not filename:\n",
        "            name = profile.get(\"name\", \"unknown\").replace(\" \", \"_\").lower()\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"resume_profile_{name}_{timestamp}.json\"\n",
        "        if not filename.lower().endswith('.json'):\n",
        "            filename += '.json'\n",
        "\n",
        "        output_dir = Path(\"output\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        filepath = output_dir / filename\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(profile, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return str(filepath)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error saving profile: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def print_profile(profile):\n",
        "    print(json.dumps(profile, indent=2, ensure_ascii=False))\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "try:\n",
        "    if not resume_text:\n",
        "        raise ValueError(\"No resume text provided\")\n",
        "\n",
        "    profile = build_profile(resume_text)\n",
        "    print_profile(profile)\n",
        "\n",
        "except Exception as e:\n",
        "    error_profile = build_error_profile(e, locals().get('resume_text'))\n",
        "    print_profile(error_profile)\n"
      ],
      "metadata": {
        "id": "8KvftnZnyNsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0cb85c-fb3f-4c7f-8ac6-9170d9ae8fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Laiba Khan\",\n",
            "  \"contact\": {\n",
            "    \"phone\": \"+92 335 2036450\",\n",
            "    \"email\": \"klaiba2406@gmail.com\",\n",
            "    \"linkedin\": \"https://www.linkedin.com/in/laibakhan00/\",\n",
            "    \"github\": \"https://github.com/laibak24\"\n",
            "  },\n",
            "  \"summary\": \"driven chatbot trained on dream interpretation books to analyze and explain user-submitted dreams.\",\n",
            "  \"skills\": {\n",
            "    \"technical\": [\n",
            "      \"flask\",\n",
            "      \"nlp\",\n",
            "      \"firebase\",\n",
            "      \"python\",\n",
            "      \"machine learning\",\n",
            "      \"gpt\",\n",
            "      \"django\",\n",
            "      \"postgresql\",\n",
            "      \"mongodb\",\n",
            "      \"typescript\",\n",
            "      \"figma\",\n",
            "      \"javascript\",\n",
            "      \"ml\",\n",
            "      \"c\",\n",
            "      \"css\",\n",
            "      \"react native\",\n",
            "      \"react\",\n",
            "      \"sql\",\n",
            "      \"html\",\n",
            "      \"ui/ux\",\n",
            "      \"mysql\"\n",
            "    ],\n",
            "    \"soft\": [\n",
            "      \"collaboration\"\n",
            "    ]\n",
            "  },\n",
            "  \"education\": [\n",
            "    \"Bachelor's in Computer Science FAST - National University of Computer and Emerging Sciences (NUCES). (2022 - Present)\",\n",
            "    \"A-Levels (Computer Science) - The City School. (2020 - 2022)\",\n",
            "    \"O-Levels (Computer Science) - The City School. (2017 - 2020)\"\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    \"/Internship Experience 1.​Software Engineering Intern​ Headstarter | (August 2024 - September 2024) o​ Developed and optimized frontend components using React, Next.js, HTML, CSS, and JavaScript o​ Integrated RESTful APIs to fetch and display dynamic data efficiently o​ Improved website performance and responsiveness, ensuring cross-browser compatibility 2.​AI Developer Intern​ Blunder Bot Technologies | (June 2025 - Present) o​ Built and optimized RAG pipelines for improved language model performance o​ Fine-tuned transformer models (e.g., GPT-2) on domain-specific data o​ Used vector databases (e.g., FAISS) for fast document retrieval​ Personal Projects ●​ PersonPicks - Movie & Book Recommendation System (Django, PostgreSQL, React, Railway, Vercel) Developed a full-stack web application that suggests movies and books based on users' MBTI type. The platform enables users to connect, follow others, and view their watchlist/readlist items, providing a personalized recommendation experience. GitHub: https://github.com/laibak24/personapicks2.0 ●​ Slogan Quality Predictor - AI-Based Slogan Evaluation (Python, Machine Learning, NLP, Streamlit) Built an AI-powered tool that evaluates the memorability and effectiveness of slogans based on multiple linguistic and psychological factors. The model assigns a quality score to slogans, assisting in branding and marketing strategies. GitHub: https://github.com/laibak24/slogan-quality-predictor ●​ MediQure - Medication Tracker App (React Native, Expo, Firebase, JavaScript/TypeScript) Developed a mobile-based medication tracker to help users manage their medication schedules. Features include timely reminders, dosage tracking, prescription management, and adherence reports to ensure users never miss a dose. GitHub: https://github.com/laibak24/Medication-Tracker ●​ SomniSense - AI-Powered Dream Interpreter (Chatbase, NLP, React/React Native, Figma) Built an AI-driven chatbot trained on dream interpretation books to analyze and explain user-submitted dreams. Designed a seamless UI/UX for an intuitive experience, leveraging Natural Language Processing (NLP) for accurate contextual insights. Developed the front-end using React/React Native and optimized chatbot responses for engagement and accuracy. GitHub: https://github.com/laibak24/SomniSense Courses & Certificates ●​ JavaScript Algorithms and Data Structures - freeCodeCamp (July 2024) ●​ Exploratory Data Analysis in SQL - DataCamp (August 2024) ●​ Data Analyst Professional - Coursera (February 2025) ●​ Structuring Machine Learning Projects - DeepLearning.AI (April 2025)\"\n",
            "  ],\n",
            "  \"projects\": [\n",
            "    {\n",
            "      \"content\": \"PersonPicks - Movie & Book Recommendation System (Django, PostgreSQL, React, Railway, Vercel)\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"Developed a full-stack web application that suggests movies and books based on users' MBTI type. The platform enables users to connect, follow others, and view their watchlist/readlist items, providing a personalized recommendation experience.\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"GitHub: https://github.com/laibak24/personapicks2.0 ●​\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"Slogan Quality Predictor - AI-Based Slogan Evaluation (Python, Machine Learning, NLP, Streamlit)\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"Built an AI-powered tool that evaluates the memorability and effectiveness of slogans based on multiple linguistic and psychological factors. The model assigns a quality score to slogans, assisting in branding and marketing strategies.\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"GitHub: https://github.com/laibak24/slogan-quality-predictor ●​\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"MediQure - Medication Tracker App (React Native, Expo, Firebase, JavaScript/TypeScript)\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"Developed a mobile-based medication tracker to help users manage their medication schedules. Features include timely reminders, dosage tracking, prescription management, and adherence reports to ensure users never miss a dose.\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"GitHub: https://github.com/laibak24/Medication-Tracker ●​\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"SomniSense - AI-Powered Dream Interpreter (Chatbase, NLP, React/React Native, Figma)\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"Built an AI-driven chatbot trained on dream interpretation books to analyze and explain user-submitted dreams. Designed a seamless UI/UX for an intuitive experience, leveraging Natural Language Processing (NLP) for accurate contextual insights. Developed the front-end using React/React Native and optimized chatbot responses for engagement and accuracy.\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"GitHub: https://github.com/laibak24/SomniSense\"\n",
            "    }\n",
            "  ],\n",
            "  \"certifications\": [\n",
            "    \"JavaScript Algorithms and Data Structures\",\n",
            "    \"Exploratory Data Analysis in SQL\",\n",
            "    \"Data Analyst Professional\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}